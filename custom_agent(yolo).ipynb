{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#installing dependancies"
      ],
      "metadata": {
        "id": "V7TAuOkaL_Ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uZR3iGJJtdDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4630aee5-8551-43c2-e6cd-69889896bff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.4/648.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.0.150 openai\n",
        "# !pip -q install duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k_oRniCLjpN",
        "outputId": "c97eeec9-6ac4-4a48-faef-9dde91f3985c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.150\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain==0.0.316\n",
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "id": "Qen9_KI3Lpiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8cDtxLIBHgQ",
        "outputId": "9d363f72-6c78-4e99-de34-b36c048e0b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "11c6ee83-892b-4e9e-ca4d-24c819b6c164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "cd64e7e6-d67f-4989-fac6-3cad8646b64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 29.0/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4yW4bHohYTm",
        "outputId": "c4eea51e-2c44-44be-f8ea-85ff56ce3594"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supervision\n",
            "  Downloading supervision-0.19.0-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/97.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.9.0.80)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
            "Installing collected packages: supervision\n",
            "Successfully installed supervision-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from IPython.display import display, Image\n",
        "import supervision as sv\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using roboflow platform to import our own curated dataset"
      ],
      "metadata": {
        "id": "gCoFB26XMFzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BSd93ZJzZZKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8da913b-b93a-4f0f-d884-8acf8a301d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.1/74.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in yolov-college-area-1 to yolov8:: 100%|██████████| 1213/1213 [00:00<00:00, 8622.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to yolov-college-area-1 in yolov8:: 100%|██████████| 60/60 [00:00<00:00, 5513.86it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow --quiet\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"QLDBSHjokw2mECFDghWS\")\n",
        "project = rf.workspace(\"adscreative\").project(\"yolov-college-area\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!yolo task=detect mode=predict model=yolov8n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5qQUCGcte2Y",
        "outputId": "5614c0e3-7394-4445-e62c-24230c983a0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 239MB/s]\n",
            "Ultralytics YOLOv8.0.196 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "Downloading https://media.roboflow.com/notebooks/examples/dog.jpeg to 'dog.jpeg'...\n",
            "100% 104k/104k [00:00<00:00, 29.0MB/s]\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 578.9ms\n",
            "Speed: 17.5ms preprocess, 578.9ms inference, 36.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd {HOME}\n",
        "# !yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source='/content/Screenshot 2024-04-10 at 10.39.36 AM.png' save=True show=True"
      ],
      "metadata": {
        "id": "N0cXFCHqZlvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('/content/yolov8n.pt')\n",
        "\n",
        "results = model(\"/content/dog.jpeg\")\n",
        "\n",
        "# print(len(results[0].boxes))\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Probs object for classification outputs\n",
        "    # result.show()  # display to screen\n",
        "    # result.save(filename='result.jpg')  # save to disk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVjEhBMGmDxc",
        "outputId": "8dcdf606-9a95-444c-a404-dca036c0aec7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 137.0ms\n",
            "Speed: 2.3ms preprocess, 137.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = list(results[0].names.values())\n",
        "class_labels\n"
      ],
      "metadata": {
        "id": "zqcjFtchk4VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f346da4-3d3e-47d2-b680-d476b2f10952"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['person',\n",
              " 'bicycle',\n",
              " 'car',\n",
              " 'motorcycle',\n",
              " 'airplane',\n",
              " 'bus',\n",
              " 'train',\n",
              " 'truck',\n",
              " 'boat',\n",
              " 'traffic light',\n",
              " 'fire hydrant',\n",
              " 'stop sign',\n",
              " 'parking meter',\n",
              " 'bench',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'dog',\n",
              " 'horse',\n",
              " 'sheep',\n",
              " 'cow',\n",
              " 'elephant',\n",
              " 'bear',\n",
              " 'zebra',\n",
              " 'giraffe',\n",
              " 'backpack',\n",
              " 'umbrella',\n",
              " 'handbag',\n",
              " 'tie',\n",
              " 'suitcase',\n",
              " 'frisbee',\n",
              " 'skis',\n",
              " 'snowboard',\n",
              " 'sports ball',\n",
              " 'kite',\n",
              " 'baseball bat',\n",
              " 'baseball glove',\n",
              " 'skateboard',\n",
              " 'surfboard',\n",
              " 'tennis racket',\n",
              " 'bottle',\n",
              " 'wine glass',\n",
              " 'cup',\n",
              " 'fork',\n",
              " 'knife',\n",
              " 'spoon',\n",
              " 'bowl',\n",
              " 'banana',\n",
              " 'apple',\n",
              " 'sandwich',\n",
              " 'orange',\n",
              " 'broccoli',\n",
              " 'carrot',\n",
              " 'hot dog',\n",
              " 'pizza',\n",
              " 'donut',\n",
              " 'cake',\n",
              " 'chair',\n",
              " 'couch',\n",
              " 'potted plant',\n",
              " 'bed',\n",
              " 'dining table',\n",
              " 'toilet',\n",
              " 'tv',\n",
              " 'laptop',\n",
              " 'mouse',\n",
              " 'remote',\n",
              " 'keyboard',\n",
              " 'cell phone',\n",
              " 'microwave',\n",
              " 'oven',\n",
              " 'toaster',\n",
              " 'sink',\n",
              " 'refrigerator',\n",
              " 'book',\n",
              " 'clock',\n",
              " 'vase',\n",
              " 'scissors',\n",
              " 'teddy bear',\n",
              " 'hair drier',\n",
              " 'toothbrush']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detections = \"\"\n",
        "for i in range(len(results[0].boxes.cls)):\n",
        "    class_index = int(results[0].boxes.cls[i])\n",
        "    confidence = results[0].boxes.conf[i]\n",
        "    label = class_labels[class_index]\n",
        "    # print(f\"Detected: {label} with confidence: {confidence}\")\n",
        "    detections+= f\"Detected: {label} with confidence: {confidence}\"\n",
        "detections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wBW-YWI_u1ov",
        "outputId": "0a3f6d68-81a8-42fe-9ab3-51feb51ec579"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Detected: person with confidence: 0.7271240949630737Detected: dog with confidence: 0.29066359996795654Detected: car with confidence: 0.2845548391342163'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('/content/yolov8n.pt')\n",
        "\n",
        "results = model.predict(source = \"/content/dog.jpeg\")\n",
        "\n",
        "# print(len(results[0].boxes))\n",
        "for result in results:\n",
        "  print(result.probs)\n",
        "result = result.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy1lcvIOrY6n",
        "outputId": "082bf7c4-f79a-46ec-aa1d-c2c84b1099a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 166.6ms\n",
            "Speed: 4.7ms preprocess, 166.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im2 = cv2.imread(\"/content/dog.jpeg\")\n",
        "results = model.predict(source=im2, save=True, save_txt=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfyusdHH36Fe",
        "outputId": "7cae23fd-c8e6-4775-b182-dc8b05315914"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 640x384 1 person, 1 car, 1 dog, 138.3ms\n",
            "Speed: 4.1ms preprocess, 138.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
            "1 label saved to runs/detect/predict3/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c02c4fa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "73bfcbb6"
      },
      "outputs": [],
      "source": [
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "Lald4gCltB4V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/dog.jpeg'"
      ],
      "metadata": {
        "id": "A0IrcOCM2edd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cv_tool(BaseTool):\n",
        "  name = \"object detector\"\n",
        "  description = \"Use this tool when given the path to an image that you would like to detect objects.\" \\\n",
        "                \"it will return a list of all detected objects. each element in the list will be in the format:\" \\\n",
        "                \"Detected: label with confidence: confidence\"\n",
        "  def _run(self,img_path):\n",
        "    from ultralytics import YOLO\n",
        "    model = YOLO('/content/yolov8n.pt')\n",
        "    results = model(img_path)\n",
        "    class_labels = list(results[0].names.values())\n",
        "\n",
        "    detections = \"\"\n",
        "    for i in range(len(results[0].boxes.cls)):\n",
        "        class_index = int(results[0].boxes.cls[i])\n",
        "        confidence = results[0].boxes.conf[i]\n",
        "        label = class_labels[class_index]\n",
        "        # print(f\"Detected: {label} with confidence: {confidence}\")\n",
        "        detections+= f\"Detected: {label} with confidence: {confidence}\"\n",
        "\n",
        "    return detections\n",
        "\n",
        "  def _arun(self, query: str):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E709myfKqWNs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#helper function"
      ],
      "metadata": {
        "id": "5OdJhR9MMk21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(img_path):\n",
        "    \"\"\"\n",
        "    Detects objects in the provided image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        str: A string with all the detected objects. Each object as 'Detected: {label} with confidence: {confidence}'.\n",
        "    \"\"\"\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "    model = YOLO('/content/yolov8n.pt')\n",
        "    results = model(img_path)\n",
        "    class_labels = list(results[0].names.values())\n",
        "\n",
        "    detections = \"\"\n",
        "    for i in range(len(results[0].boxes.cls)):\n",
        "        class_index = int(results[0].boxes.cls[i])\n",
        "        confidence = results[0].boxes.conf[i]\n",
        "        label = class_labels[class_index]\n",
        "        # print(f\"Detected: {label} with confidence: {confidence}\")\n",
        "        detections+= f\"Detected: {label} with confidence: {confidence}\"\n",
        "\n",
        "    return detections\n"
      ],
      "metadata": {
        "id": "mV-yXK5NqWLn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_objects(img_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "fZZLwLrhC8KV",
        "outputId": "ee392f6d-0496-49ff-88dd-291a0739e3b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 439.4ms\n",
            "Speed: 4.2ms preprocess, 439.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Detected: person with confidence: 0.7271240949630737Detected: dog with confidence: 0.29066359996795654Detected: car with confidence: 0.2845548391342163'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [cv_tool()]\n",
        "\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=3,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent = \"chat-conversational-react-description\",\n",
        "    tools = tools,\n",
        "    llm = turbo_llm,\n",
        "    max_iterations = 3,\n",
        "    verbose=True,\n",
        "    memory=conversational_memory,\n",
        "    early_stopping_method = 'generate'\n",
        ")"
      ],
      "metadata": {
        "id": "VqjeZU4dqWJf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/bottle.jpeg'"
      ],
      "metadata": {
        "id": "8-vMC4InNqIe"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"please tell me the items found detected in this image\"\n",
        "response = agent.run(f'{user_question}, this is the image path: {img_path}')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "sSSJg1wJqWHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081c146d-5ec5-4318-9d3c-56b4836c40b2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"object detector\",\n",
            "    \"action_input\": \"/content/bottle.jpeg\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/bottle.jpeg: 640x480 1 bottle, 2 refrigerators, 169.7ms\n",
            "Speed: 5.4ms preprocess, 169.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mDetected: bottle with confidence: 0.5924678444862366Detected: refrigerator with confidence: 0.5823495388031006Detected: refrigerator with confidence: 0.283882200717926\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Detected: bottle with confidence: 0.5924678444862366, Detected: refrigerator with confidence: 0.5823495388031006, Detected: refrigerator with confidence: 0.283882200717926\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Detected: bottle with confidence: 0.5924678444862366, Detected: refrigerator with confidence: 0.5823495388031006, Detected: refrigerator with confidence: 0.283882200717926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0eH6eadNvg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}